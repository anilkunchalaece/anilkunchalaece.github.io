<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-05-07T20:31:54+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Anil Kunchala</title><subtitle>a &lt;i&gt; Tinkerers &lt;/i&gt; log about his experiments with few things he came across</subtitle><author><name>Anil Kunchala</name></author><entry><title type="html">Fine-tuning Custom Object Detector Using Pytorch</title><link href="http://localhost:4000/blog/computer-vision/2025/05/06/custom-object-train.html" rel="alternate" type="text/html" title="Fine-tuning Custom Object Detector Using Pytorch" /><published>2025-05-06T00:00:00+01:00</published><updated>2025-05-06T00:00:00+01:00</updated><id>http://localhost:4000/blog/computer-vision/2025/05/06/custom-object-train</id><content type="html" xml:base="http://localhost:4000/blog/computer-vision/2025/05/06/custom-object-train.html"><![CDATA[<p>In this post, I will describe how to fine-tune and pretrained object detector model using pytorch.</p>

<h4 id="data-requirements">Data Requirements</h4>
<p>For Object Detection Training and Test line, you Need</p>
<ul>
  <li>Images</li>
  <li>No of Classes you want to detect in the images [One image may contain multiple classes]</li>
  <li>Bounding Boxes of Each Object with in Image [These may have different formats based on annotation tool]</li>
</ul>

<h4 id="what-you-need-to-do">What You need to do</h4>
<p>Most of the work you do can be split into 3 stages</p>
<ol>
  <li>Custom Dataset Class for your data</li>
  <li>Model Modification</li>
  <li>Train and Test Scripts</li>
</ol>

<h4 id="custom-dataset-class">Custom Dataset Class</h4>
<ul>
  <li>Create an custom dataset class (lets call it CustomDatasetClass) <strong>inherited</strong> from <code class="language-plaintext highlighter-rouge">torch.utils.data.dataset</code> class.</li>
  <li>In the custom class create the following methods. These will be replaced in parent <code class="language-plaintext highlighter-rouge">torch.utils.data.datset</code> class
    <ul>
      <li><code class="language-plaintext highlighter-rouge">__getitem(self,idx)__</code>
        <ul>
          <li>This is used to get the each item for training and testing</li>
          <li>Include the code to do any preprocessing for data - for ex, converting image and class labels to tensors</li>
          <li><strong>For Object Detection</strong>
            <ul>
              <li><code class="language-plaintext highlighter-rouge">__getitem__</code> should return following
                <ul>
                  <li><strong>image :</strong> should be a tensor of shape [Channels, Height, Width]</li>
                  <li><strong>targets :</strong> list of dictionary containing
                    <ul>
                      <li><em>boxes :</em> ground truth bounding boxes in <code class="language-plaintext highlighter-rouge">[x1,y1,x2,y2]</code> format</li>
                      <li><em>labels :</em> class label for each bounding box</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">__len(self)__</code>
        <ul>
          <li>This is used to get the total number of samples in the dataset</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Once you create these two methods, we can use this class to load the dataset suitable for pytorch dataloader for both training and testing</p>

<p>References</p>
<ul>
  <li><a href="https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html">https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html</a></li>
  <li><a href="https://docs.pytorch.org/vision/stable/_modules/torchvision/models/detection/faster_rcnn.html#FasterRCNN_ResNet50_FPN_V2_Weights">https://docs.pytorch.org/vision/stable/_modules/torchvision/models/detection/faster_rcnn.html#FasterRCNN_ResNet50_FPN_V2_Weights</a></li>
</ul>

<h4 id="model-modifications">Model Modifications</h4>
<p>Instead of creating model from scratch, We can use one of the pretrained models from pytorch and modify the number of classes in the classifier head to match our use case.</p>

<p>Following is the comparision of easy to use pytorch models</p>

<table>
  <thead>
    <tr>
      <th>Model Name</th>
      <th>Backbone</th>
      <th>Type</th>
      <th>Speed</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>fasterrcnn_resnet50_fpn</td>
      <td>ResNet50 + FPN</td>
      <td>Two Stage</td>
      <td>Medium</td>
      <td>High</td>
    </tr>
    <tr>
      <td>fasterrcnn_mobilenet_v3_large_fpn</td>
      <td>MobileNet + FPN</td>
      <td>Two Stage</td>
      <td>Fast</td>
      <td>Medium</td>
    </tr>
    <tr>
      <td>retinanet_resnet50_fpn</td>
      <td>ResNet50 + FPN</td>
      <td>One Stage</td>
      <td>Fasterthan FastRCNN</td>
      <td>Decent</td>
    </tr>
  </tbody>
</table>

<p><strong>Faster RCNN</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.models.detection.faster_rcnn</span> <span class="kn">import</span> <span class="n">FastRCNNPredictor</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">detection</span><span class="p">.</span><span class="n">fasterrcnn_resnet50_fpn</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">in_features</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">roi_heads</span><span class="p">.</span><span class="n">box_predictor</span><span class="p">.</span><span class="n">cls_score</span><span class="p">.</span><span class="n">in_features</span>
<span class="n">model</span><span class="p">.</span><span class="n">roi_heads</span><span class="p">.</span><span class="n">box_predictor</span> <span class="o">=</span> <span class="n">FastRCNNPredictor</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>MobileNet</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.models.detection.faster_rcnn</span> <span class="kn">import</span> <span class="n">FastRCNNPredictor</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">detection</span><span class="p">.</span><span class="n">fasterrcnn_mobilenet_v3_large_fpn</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">in_features</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">roi_heads</span><span class="p">.</span><span class="n">box_predictor</span><span class="p">.</span><span class="n">cls_score</span><span class="p">.</span><span class="n">in_features</span>
<span class="n">model</span><span class="p">.</span><span class="n">roi_heads</span><span class="p">.</span><span class="n">box_predictor</span> <span class="o">=</span> <span class="n">FastRCNNPredictor</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>RetinaNet</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.models.detection.retinanet</span> <span class="kn">import</span> <span class="n">RetinaNetClassificationHead</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">detection</span><span class="p">.</span><span class="n">retinanet_resnet50_fpn</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">in_channels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">head</span><span class="p">.</span><span class="n">classification_head</span><span class="p">.</span><span class="n">conv</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">in_channels</span>  <span class="c1"># usually 256
</span><span class="n">num_anchors</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">head</span><span class="p">.</span><span class="n">classification_head</span><span class="p">.</span><span class="n">num_anchors</span>  <span class="c1"># usually 9
</span>
<span class="n">model</span><span class="p">.</span><span class="n">head</span><span class="p">.</span><span class="n">classification_head</span> <span class="o">=</span> <span class="n">RetinaNetClassificationHead</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">num_anchors</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Blog&quot;]" /><summary type="html"><![CDATA[Fine-Tuning Custom Object Detector using Pytorch]]></summary></entry><entry><title type="html">Actor-Centric Spatio-Temporal Feature Extraction for Action Recognition</title><link href="http://localhost:4000/phd/publication/projects/2024/08/02/actor-centric-paper.html" rel="alternate" type="text/html" title="Actor-Centric Spatio-Temporal Feature Extraction for Action Recognition" /><published>2024-08-02T00:00:00+01:00</published><updated>2024-08-02T00:00:00+01:00</updated><id>http://localhost:4000/phd/publication/projects/2024/08/02/actor-centric-paper</id><content type="html" xml:base="http://localhost:4000/phd/publication/projects/2024/08/02/actor-centric-paper.html"><![CDATA[<h2 id="overview">Overview</h2>
<p><strong>Title:</strong> Actor-Centric Spatio-Temporal Feature Extraction for Action Recognition</p>

<p><strong>Published in:</strong> 2023 International Conference on Computer Vision and Image Processing (CVIP 2023)</p>

<p><img src="http://localhost:4000/assets/images/cvip_2023/actor_centric.png" alt="image" />
<strong>Proposed Actor-Centric Tubelet Action Recognition Network Compared to Existing Frame-based Action Recognition Networks</strong></p>

<h2 id="abstract">Abstract</h2>
<p>Action understanding involves the recognition and detection of specific actions within videos. This crucial task in computer vision gained significant attention due to its multitude of applications across various domains. The current action detection models, inspired by 2D object detection methods, employ two-stage architectures. The first stage is to extract actor-centric video sub-clips, i.e. tubelets of individuals, and the second stage is to classify these tubelets using action recognition networks. The majority of these recognition models utilize a frame-level pre-trained 3D Convolutional Neural Networks (3D CNN) to extract spatio-temporal features of a given tubelet. This, however, results in suboptimal spatio-temporal feature representation for action recognition, primarily because the actor typically occupies a relatively small area in the frame.</p>

<p>This work proposes the use of actor-centric tubelets instead of frames to learn spatio-temporal feature representation for action recognition. We present an empirical study of the actor-centric tubelet and frame-level action recognition models and propose a baseline for actor-centric action recognition. We evaluated the proposed method on the state-of-the-art C3D, I3D, and SlowFast 3D CNN architectures using the NTURGBD dataset. Our results demonstrate that the actor-centric feature extractor consistently outperforms the frame-level and large pre-trained fine-tuned models. The source code for the tubelet generation is available at <a href="https://github.com/anilkunchalaece/ntu_tubelet_parser"> https://github.com/anilkunchalaece/ntu_tubelet_parser.</a></p>

<h2 id="key-contributions">Key Contributions</h2>
<ul>
  <li>We conducted an extensive comparative study of frame-based and actor-centric tubelet-based feature extraction for action classification.</li>
  <li>We propose an actor-centric tubelet-based backbone that enhances spatio-temporal feature extraction for action classification and detection compared to the existing frame-level models.</li>
</ul>

<h2 id="qualitative-results">Qualitative Results</h2>
<p><img src="http://localhost:4000/assets/images/cvip_2023/results.png" alt="image" /></p>

<h2 id="publication-link-and-code">Publication Link and Code</h2>
<ul>
  <li><a href="https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1454&amp;context=scschcomcon" target="_blank">Read the full paper</a></li>
  <li><a href="https://drive.google.com/file/d/1MENLg1BWRT_PEb_m8GerouIeSCwkI3me/view?usp=drive_link" target="_blank">CVIP 2023 Presentation Slides</a></li>
  <li><a href="https://github.com/anilkunchalaece/mmaction2-af_new" target="_blank">Source Code</a></li>
</ul>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Projects&quot;]" /><summary type="html"><![CDATA[Actor-Centric Spatio-Temporal Feature Extraction for Action Recognition]]></summary></entry><entry><title type="html">Towards A Framework for Privacy-Preserving Pedestrian Analysis</title><link href="http://localhost:4000/phd/publication/projects/2024/02/19/privacy-framework-paper.html" rel="alternate" type="text/html" title="Towards A Framework for Privacy-Preserving Pedestrian Analysis" /><published>2024-02-19T00:00:00+00:00</published><updated>2024-02-19T00:00:00+00:00</updated><id>http://localhost:4000/phd/publication/projects/2024/02/19/privacy-framework-paper</id><content type="html" xml:base="http://localhost:4000/phd/publication/projects/2024/02/19/privacy-framework-paper.html"><![CDATA[<h2 id="overview">Overview</h2>
<p><strong>Title:</strong> Towards A Framework for Privacy-Preserving Pedestrian Analysis</p>

<p><strong>Published in:</strong> 2023 IEEE CVF Winter Conference on Applications of Computer Vision (WACV 2023)</p>

<p><img src="http://localhost:4000/assets/images/wacv2023/wacv2023-results.png" alt="image" />
<strong>Qualitative results for the proposed and baseline methods</strong></p>

<h2 id="abstract">Abstract</h2>
<p>The design of pedestrian-friendly infrastructures plays a crucial role in creating sustainable transportation in urban environments. Analyzing pedestrian behaviour in response to existing infrastructure is pivotal to planning, maintaining, and creating more pedestrian-friendly facilities. Many approaches have been proposed to extract such behaviour by applying deep learning models to video data. Video data, however, includes an broad spectrum of privacy-sensitive information about individuals, such as their location at a given time or who they are with. Most of the existing mod-
els use privacy-invasive methodologies to track, detect, and analyse individual or group pedestrian behaviour patterns.</p>

<p>As a step towards privacy-preserving pedestrian analysis, this paper introduces a framework to anonymize all pedestrians before analyzing their behaviors. The proposed framework leverages recent developments in 3D wireframe reconstruction and digital in-painting to represent pedestrians with quantitative wireframes by removing their images while preserving pose, shape, and background scene context. To evaluate the proposed framework, a generic metric is introduced for each of privacy and utility. Experimental evaluation on widely-used datasets shows that the proposed framework outperforms traditional and state-of-the-art image filtering approaches by generating best privacy utility
trade-off.</p>

<h2 id="key-contributions">Key Contributions</h2>
<ul>
  <li>A novel end-to-end framework is introduced to generate a privacy-enhanced version of a given video or
image sequence.</li>
  <li>Both a generic utility and statistical similarity-based privacy metrics are proposed to evaluate the privacy utility trade-off.</li>
</ul>

<h2 id="overview-of-the-proposed-framework">Overview of the proposed framework</h2>
<p><img src="http://localhost:4000/assets/images/wacv2023/wacv2023-framework.png" alt="image" /></p>

<h2 id="privacy-utility-trade-off">Privacy-Utility Trade-off</h2>
<p><img src="http://localhost:4000/assets/images/wacv2023/wacv2023-results-plots.png" alt="image" /></p>

<h2 id="publication-link-and-code">Publication Link and Code</h2>
<ul>
  <li><a href="https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1411&amp;context=scschcomcon" target="_blank">Read the full paper</a></li>
  <li><a href="https://www.youtube.com/watch?v=Ke5vPS9fwUA" target="_blank">WACV 2023 Presentation Video</a></li>
  <li><a href="https://drive.google.com/file/d/1ZROKNr6W2BqqKL1qUM2tOAF-ckC4zrZi/view?usp=sharing" target="_blank">WACV 2023 Presentation Slides</a></li>
  <li><a href="https://drive.google.com/file/d/1DIcMZizTPD4LhC8Ir7pYvc7vNcjhyS4v/view?usp=sharing" target="_blank">WACV 2023 Poster</a></li>
  <li><a href="https://github.com/anilkunchalaece/privacyFramework" target="_blank">Source Code</a></li>
</ul>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Projects&quot;]" /><summary type="html"><![CDATA[Towards A Framework for Privacy-Preserving Pedestrian Analysis]]></summary></entry><entry><title type="html">Bray To Graystones- Ireland</title><link href="http://localhost:4000/hiking/travel/2024/02/18/bray.html" rel="alternate" type="text/html" title="Bray To Graystones- Ireland" /><published>2024-02-18T00:00:00+00:00</published><updated>2024-02-18T00:00:00+00:00</updated><id>http://localhost:4000/hiking/travel/2024/02/18/bray</id><content type="html" xml:base="http://localhost:4000/hiking/travel/2024/02/18/bray.html"><![CDATA[<p>Route</p>

<p><img src="http://localhost:4000/assets/maps/bray-greystones-cliff-walk.png" alt="image" /></p>

<p>Few Snaps
<img src="http://localhost:4000/assets/images/bray/20240218_120542.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/bray/20240218_122356.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/bray/20240218_120711.jpg" alt="image" /></p>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Travel&quot;]" /><summary type="html"><![CDATA[A Trip to Bray]]></summary></entry><entry><title type="html">SMPL-based 3D Pedestrian Pose Prediction</title><link href="http://localhost:4000/phd/publication/projects/2024/02/15/smpl-paper.html" rel="alternate" type="text/html" title="SMPL-based 3D Pedestrian Pose Prediction" /><published>2024-02-15T00:00:00+00:00</published><updated>2024-02-15T00:00:00+00:00</updated><id>http://localhost:4000/phd/publication/projects/2024/02/15/smpl-paper</id><content type="html" xml:base="http://localhost:4000/phd/publication/projects/2024/02/15/smpl-paper.html"><![CDATA[<h2 id="overview">Overview</h2>
<p><strong>Title:</strong> SMPL-Based 3D Pedestrian Pose Prediction</p>

<p><strong>Published in:</strong> 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</p>

<p><img src="http://localhost:4000/assets/images/fg2021/ADV_SMPL_AWARE.png" alt="image" />
<strong>Adversarial SMPL-based Recurrent Neural Network Architecture</strong></p>

<h2 id="abstract">Abstract</h2>
<p>In 3D pedestrian pose prediction, joint-rotation-based pose representation is extensively used due to the unconstrained degree of freedom for each joint and its ability to regress the 3D statistical wireframe. However, all the existing joint-rotation-based pose prediction approaches ignore the centrality of the distinct pose parameter components and are consequently prone to suffer from error accumulation along the kinematic chain, which results in unnatural human poses. In joint-rotationbased pose prediction, Skinned Multi-Person Linear (SMPL) parameters are widely used to represent pedestrian pose.</p>

<p>In this work, a novel SMPL-based pose prediction network is proposed to address the centrality of each SMPL component by distributing the network weights among them. Furthermore, to constrain the network to generate only plausible human poses, an adversarial training approach is employed. The effectiveness of the proposed network is evaluated using the PedX and BEHAVE datasets. The proposed approach significantly outperforms state-of-the-art methods with improved prediction accuracy and generates plausible human pose predictions.</p>

<h2 id="key-contributions">Key Contributions</h2>
<ul>
  <li>We designed a multi-layer perception generative adversarial network to penalize the network for unnatural poses while allowing natural one.</li>
  <li>The SMPL-based architecture is proposed to address the centrality of global rotation and translation parameters with respect to the pose parameters</li>
</ul>

<h2 id="qualitative-results">Qualitative Results</h2>
<p><img src="http://localhost:4000/assets/images/fg2021/ADV_SMPL_AWARE_Results.png" alt="image" /></p>

<h2 id="publication-link-and-code">Publication Link and Code</h2>
<ul>
  <li><a href="https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1374&amp;context=scschcomcon" target="_blank">Read the full paper</a></li>
  <li><a href="https://youtu.be/OeOTEcbYsrI" target="_blank">FG 2021 Presentation Video</a></li>
  <li><a href="https://drive.google.com/file/d/1yl6l8_vDKse4vCQ_LtTk6dgMY3LTmAqw/view?usp=sharing" target="_blank">FG 2021 Presentation Slides</a></li>
  <li><a href="https://drive.google.com/file/d/1K3FvySkvguGt_yD-mFijZyALVjKBzN14/view?usp=sharing" target="_blank">FG 2021 Poster</a></li>
  <li><a href="https://github.com/anilkunchalaece/ADV-SA-LSTM" target="_blank">Source Code</a></li>
</ul>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Projects&quot;]" /><summary type="html"><![CDATA[Adversarial SMPL based 3D Pedestrian Pose Prediction]]></summary></entry><entry><title type="html">Mourne Mountains - Northern Ireland</title><link href="http://localhost:4000/hiking/travel/2022/05/28/Mourne-Mountains.html" rel="alternate" type="text/html" title="Mourne Mountains - Northern Ireland" /><published>2022-05-28T00:00:00+01:00</published><updated>2022-05-28T00:00:00+01:00</updated><id>http://localhost:4000/hiking/travel/2022/05/28/Mourne-Mountains</id><content type="html" xml:base="http://localhost:4000/hiking/travel/2022/05/28/Mourne-Mountains.html"><![CDATA[<p>Route</p>

<p><img src="http://localhost:4000/assets/maps/21-may-hillwalkerclub-mourne-mountains.png" alt="image" /></p>

<p>Few Snaps
<img src="http://localhost:4000/assets/images/mourne/IMG_20220521_110320.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/mourne/IMG_20220521_131018.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/mourne/IMG_20220521_140154.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/mourne/IMG_20220521_160438.jpg" alt="image" /></p>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Travel&quot;]" /><summary type="html"><![CDATA[A Trip to Mourne Mountains]]></summary></entry><entry><title type="html">Wicklow Mountains, Tonelagee - Ireland</title><link href="http://localhost:4000/hiking/travel/2022/04/24/Tonelagee.html" rel="alternate" type="text/html" title="Wicklow Mountains, Tonelagee - Ireland" /><published>2022-04-24T00:00:00+01:00</published><updated>2022-04-24T00:00:00+01:00</updated><id>http://localhost:4000/hiking/travel/2022/04/24/Tonelagee</id><content type="html" xml:base="http://localhost:4000/hiking/travel/2022/04/24/Tonelagee.html"><![CDATA[<p>Route</p>

<p><img src="http://localhost:4000/assets/maps/glendalough-upper-lake-tonelagee.png" alt="image" /></p>

<p>Few Snaps
<img src="http://localhost:4000/assets/images/tonelagee/IMG_20220424_140145.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/tonelagee/IMG_20220424_140258.jpg" alt="image" /></p>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Travel&quot;]" /><summary type="html"><![CDATA[A Trip to Wicklow Mountains]]></summary></entry><entry><title type="html">Mount Leinster- Ireland</title><link href="http://localhost:4000/hiking/travel/2022/04/16/mountLeinster.html" rel="alternate" type="text/html" title="Mount Leinster- Ireland" /><published>2022-04-16T00:00:00+01:00</published><updated>2022-04-16T00:00:00+01:00</updated><id>http://localhost:4000/hiking/travel/2022/04/16/mountLeinster</id><content type="html" xml:base="http://localhost:4000/hiking/travel/2022/04/16/mountLeinster.html"><![CDATA[<p>Route</p>

<p><img src="http://localhost:4000/assets/maps/mount-leinster.png" alt="image" /></p>

<p>Few Snaps
<img src="http://localhost:4000/assets/images/leinster/IMG_20220416_133145.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/leinster/IMG_20220416_133331.jpg" alt="image" /></p>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Travel&quot;]" /><summary type="html"><![CDATA[A Trip to Mount Leinster]]></summary></entry><entry><title type="html">Xnect - Installation and running demo</title><link href="http://localhost:4000/projects/blog/2020/10/28/XNect-setup.html" rel="alternate" type="text/html" title="Xnect - Installation and running demo" /><published>2020-10-28T00:00:00+00:00</published><updated>2020-10-28T00:00:00+00:00</updated><id>http://localhost:4000/projects/blog/2020/10/28/XNect-setup</id><content type="html" xml:base="http://localhost:4000/projects/blog/2020/10/28/XNect-setup.html"><![CDATA[<p>In this Post I will show how to install <a href="https://gvv.mpi-inf.mpg.de/projects/XNect/">XNect</a> and Run the demo in C/C++ Library provided by xNect Team.</p>

<p>In summary you need to install following dependencies</p>
<ol>
  <li>OpenCV 3.4.11 - <em>recommended , dont use Opencv 4</em></li>
  <li>Protobuf</li>
  <li>Boost 1.58.0</li>
  <li>CUDA - <em>8 or 9 recommened but I tested with 10</em></li>
  <li>Caffe - <em>need to make few modifications before installation</em></li>
</ol>

<h4 id="machine-configuration">Machine Configuration</h4>
<p>I tested using machine with following configuration</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=19.10
DISTRIB_CODENAME=eoan
x86_64
Kernal - 5.3.0-64-generic 
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/images/nvidia_info.jpg" alt="image" /></p>

<h4 id="1-installing-opencv-and-contrib-libs">1. installing openCV and contrib libs</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  wget <span class="nt">-c</span>  <span class="nt">-O</span> opencv-3.4.11.tar.gz https://github.com/opencv/opencv/archive/3.4.11.tar.gz       
  <span class="nb">tar </span>xvzf opencv-3.4.11.tar.gz

  wget <span class="nt">-c</span> <span class="nt">-O</span> opencv_contrib-3.4.11 https://github.com/opencv/opencv_contrib/archive/3.4.11.tar.gz  
  <span class="nb">tar </span>xvzf opencv_contrib-3.4.11.tar.gz  

  <span class="nb">rm </span>opencv-3.4.11.tar.gz opencv_contrib-3.4.11.tar.gz

  <span class="nb">cd </span>opencv-3.4.11
  <span class="nb">mkdir </span>build
  <span class="nb">cd </span>build

  <span class="c"># Run cmake - Check XNect readme.md for flags</span>
  cmake <span class="nt">-D</span> <span class="nv">CMAKE_BUILD_TYPE</span><span class="o">=</span>RELEASE <span class="nt">-D</span> <span class="nv">CMAKE_INSTALL_PREFIX</span><span class="o">=</span>/usr/local <span class="nt">-D</span> <span class="nv">INSTALL_C_EXAMPLES</span><span class="o">=</span>ON <span class="nt">-D</span> <span class="nv">INSTALL_PYTHON_EXAMPLES</span><span class="o">=</span>ON <span class="nt">-D</span> <span class="nv">OPENCV_EXTRA_MODULES_PATH</span><span class="o">=</span>/home/anil/Documents/xNect/opencv_contrib-3.4.11/modules <span class="nt">-D</span> <span class="nv">BUILD_EXAMPLES</span><span class="o">=</span>ON <span class="nt">-D</span> <span class="nv">WITH_TBB</span><span class="o">=</span>ON <span class="nt">-D</span> <span class="nv">BUILD_NEW_PYTHON_SUPPORT</span><span class="o">=</span>ON <span class="nt">-D</span> <span class="nv">WITH_V4L</span><span class="o">=</span>ON <span class="nt">-D</span> <span class="nv">WITH_QT</span><span class="o">=</span>ON <span class="nt">-D</span> <span class="nv">WITH_OPENGL</span><span class="o">=</span>ON <span class="nt">-D</span> <span class="nv">BUILD_TIFF</span><span class="o">=</span>ON ..

  make <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span>
  <span class="nb">sudo </span>make <span class="nb">install
  sudo </span>ldconfig
</code></pre></div></div>

<p>Once installtion completed, check it</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
<span class="o">&gt;&gt;&gt;</span> import cv2
<span class="o">&gt;&gt;&gt;</span> cv2.__version__
<span class="s1">'3.4.11'</span>
</code></pre></div></div>

<h4 id="2-installing-protobuf">2. installing protobuf</h4>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>autoconf automake libtool curl make g++ unzip <span class="nt">-y</span>
git clone https://github.com/google/protobuf.git
<span class="nb">cd </span>protobuf
git submodule update <span class="nt">--init</span> <span class="nt">--recursive</span>
./autogen.sh
./configure <span class="nv">CFLAGS</span><span class="o">=</span><span class="s2">"-fPIC"</span> <span class="nv">CXXFLAGS</span><span class="o">=</span><span class="s2">"-fPIC"</span>  <span class="o">(</span>Why ? I dont know - Check XNect Readme<span class="o">)</span>
<span class="nb">sudo </span>make <span class="nb">install
sudo </span>ldconfig
</code></pre></div></div>

<h4 id="3-installing-boost-1580">3. installing boost 1.58.0</h4>
<p>download  boost from <a href="https://www.boost.org/users/history/">here</a></p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tar </span>xvzf boost-1.58.tar
<span class="nb">cd </span>boost-1.58/tools/build
/bootstrap.sh <span class="nt">--prefix</span><span class="o">=</span>path/to/installation/prefix
./b2 <span class="nb">install</span>
</code></pre></div></div>
<ul>
  <li>Add PREFIX/bin to your PATH environment variable.</li>
</ul>

<h4 id="4-installing-caffe">4. installing caffe</h4>
<p>BVLC GitHub repository and grab the latest version of Caffe</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/BVLC/caffe.git
<span class="nb">cd </span>caffe
<span class="c">#copy make file </span>
<span class="nb">cp </span>Makefile.config.example Makefile.config
</code></pre></div></div>

<p>and make the following edits to <em>Makefile.config</em> - these are used to specify that we need to install caffe with GPU and CUDA support</p>

<div class="language-config highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cuDNN acceleration switch (uncomment to build with cuDNN).
</span><span class="n">USE_CUDNN</span> := <span class="m">1</span>

<span class="c"># Uncomment if you're using OpenCV 3
</span><span class="n">OPENCV_VERSION</span> := <span class="m">3</span>

<span class="c"># We need to be able to find Python.h and numpy/arrayobject.h.
</span><span class="n">PYTHON_INCLUDE</span> := /<span class="n">usr</span>/<span class="n">include</span>/<span class="n">python2</span>.<span class="m">7</span> \
        /<span class="n">usr</span>/<span class="n">local</span>/<span class="n">lib</span>/<span class="n">python2</span>.<span class="m">7</span>/<span class="n">dist</span>-<span class="n">packages</span>/<span class="n">numpy</span>/<span class="n">core</span>/<span class="n">include</span>

<span class="c"># Uncomment to support layers written in Python (will link against    Python libs)
</span><span class="n">WITH_PYTHON_LAYER</span> := <span class="m">1</span>

<span class="c"># Whatever else you find you need goes here.
</span><span class="n">INCLUDE_DIRS</span> := $(<span class="n">PYTHON_INCLUDE</span>) /<span class="n">usr</span>/<span class="n">local</span>/<span class="n">include</span> /<span class="n">usr</span>/<span class="n">include</span>/<span class="n">hdf5</span>/<span class="n">serial</span>
<span class="n">LIBRARY_DIRS</span> := $(<span class="n">PYTHON_LIB</span>) /<span class="n">usr</span>/<span class="n">local</span>/<span class="n">lib</span> /<span class="n">usr</span>/<span class="n">lib</span> /<span class="n">usr</span>/<span class="n">lib</span>/<span class="n">x86_64</span>-<span class="n">linux</span>-<span class="n">gnu</span>/<span class="n">hdf5</span>/<span class="n">serial</span>/
</code></pre></div></div>

<p>and based on the cuda version you are using uncomment architectures</p>

<div class="language-config highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># CUDA architecture setting: going with all of them.
# For CUDA &lt; 6.0, comment the *_50 through *_61 lines for compatibility.
# For CUDA &lt; 8.0, comment the *_60 and *_61 lines for compatibility.
# For CUDA &gt;= 9.0, comment the *_20 and *_21 lines for compatibility.
</span><span class="n">CUDA_ARCH</span> := 
		<span class="c"># -gencode arch=compute_20,code=sm_20 \
</span>		<span class="c"># -gencode arch=compute_20,code=sm_21 \
</span>		-<span class="n">gencode</span> <span class="n">arch</span>=<span class="n">compute_30</span>,<span class="n">code</span>=<span class="n">sm_30</span> \
		-<span class="n">gencode</span> <span class="n">arch</span>=<span class="n">compute_35</span>,<span class="n">code</span>=<span class="n">sm_35</span> \
		-<span class="n">gencode</span> <span class="n">arch</span>=<span class="n">compute_50</span>,<span class="n">code</span>=<span class="n">sm_50</span> \
		-<span class="n">gencode</span> <span class="n">arch</span>=<span class="n">compute_52</span>,<span class="n">code</span>=<span class="n">sm_52</span> \
		-<span class="n">gencode</span> <span class="n">arch</span>=<span class="n">compute_60</span>,<span class="n">code</span>=<span class="n">sm_60</span> \
		-<span class="n">gencode</span> <span class="n">arch</span>=<span class="n">compute_61</span>,<span class="n">code</span>=<span class="n">sm_61</span> \
		-<span class="n">gencode</span> <span class="n">arch</span>=<span class="n">compute_61</span>,<span class="n">code</span>=<span class="n">compute_61</span>
</code></pre></div></div>
<h5 id="41-removing-clip-layer-and-adding-other-necessary-files">4.1 Removing clip layer and adding other necessary files</h5>
<p>I’m just copy + pasting steps/info in XNect readme.md document. Please refer document for more info (I’m not sure whether I can add that Info here are not due to licencing issues)</p>
<ol>
  <li>Copy folders from XNect source to caffe (Include and src)</li>
  <li>Make necessary changes to <em>caffe.proto</em> ( Need to add extra layers )</li>
  <li>Add new layer description</li>
  <li>Remove cliplayer from caffe - incluing .hpp , .cpp files</li>
</ol>

<h5 id="42-compile-caffe">4.2 compile caffe</h5>
<p>API comes with gcc-6.4 support , I’m unable to compile this with current gcc in my system i.e gcc-9, so In order to specify different compiler for gcc<br />
<a href="https://stackoverflow.com/questions/17275348/how-to-specify-new-gcc-path-for-cmake">ref</a></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">CC</span><span class="o">=</span>/usr/local/bin/gcc-6.4
<span class="nb">export </span><span class="nv">CXX</span><span class="o">=</span>/usr/local/bin/g++-6.4
<span class="c">#cmake /path/to/your/project</span>
<span class="c">#make</span>
</code></pre></div></div>
<p>Run cmake and make for caffe</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cmake <span class="nt">-DProtobuf_LIBRARY_DEBUG</span><span class="o">=</span>/home/anil/Documents/xNect/protobuf/src/.libs/libprotobuf.so <span class="nt">-DProtobuf_PROTOC_EXECUTABLE</span><span class="o">=</span>/home/anil/Documents/xNect/protobuf/src/.libs/protoc <span class="nt">-DProtobuf_LIBRARY_RELEASE</span><span class="o">=</span>/home/anil/Documents/xNect/protobuf/src/.libs/libprotobuf.a  <span class="nt">-DProtobuf_LITE_LIBRARY_RELEASE</span><span class="o">=</span>/home/anil/Documents/xNect/protobuf/src/.libs/libprotobuf-lite.a <span class="nt">-DBOOST_INCLUDEDIR</span><span class="o">=</span>/usr/include/boost/  <span class="nt">-DBOOST_LIBRARYDIR</span><span class="o">=</span>/home/anil/Documents/xNect/boost/lib <span class="nt">-DProtobuf_INCLUDE_DIR</span><span class="o">=</span>/home/anil/Documents/xNect/protobuf/src/ <span class="nt">-DProtobuf_PROTOC_LIBRARY_RELEASE</span><span class="o">=</span>/home/anil/Documents/xNect/protobuf/src/.libs/libprotoc.so ..

make all <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="o">&amp;&amp;</span> make <span class="nb">test</span> <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="o">&amp;&amp;</span> make runtest <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="o">&amp;&amp;</span> make pycaffe <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span>
</code></pre></div></div>

<p>Once done, Add follwing to .bashrc</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export PYTHONPATH=/home/anil/Documents/xNect/caffe/python:$PYTHONPATH
</code></pre></div></div>

<p>check caffe</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python
<span class="o">&gt;&gt;&gt;</span> import caffe
</code></pre></div></div>

<h5 id="build-project-and-run-demo">build project and run demo</h5>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cmake <span class="nt">-DOpenCV_DIR</span><span class="o">=</span>/usr/share/opencv  <span class="nt">-DCaffe_DIR</span><span class="o">=</span>/home/anil/Documents/xNect/caffe/build ..
make <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span>
<span class="nb">cd</span> ../bin/Release
./XNECT
</code></pre></div></div>

<h5 id="references">References</h5>
<ol>
  <li>I found a nice article about <a href="https://chunml.github.io/ChunML.github.io/project/Installing-Caffe-Ubuntu/">installing caffe in ubuntu</a>.</li>
  <li><a href="https://gist.github.com/diegopacheco/cd795d36e6ebcd2537cd18174865887b">Protobuf installation</a></li>
  <li><a href="https://www.boost.org/doc/libs/1_66_0/more/getting_started/unix-variants.html">Boost 1.58 Installation</a></li>
</ol>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Blog&quot;]" /><summary type="html"><![CDATA[In this post I will explain how to setup XNect and run the demo provided in the example]]></summary></entry><entry><title type="html">Snowdonia - United Kingdom</title><link href="http://localhost:4000/hiking/travel/2019/10/15/snowdonia.html" rel="alternate" type="text/html" title="Snowdonia - United Kingdom" /><published>2019-10-15T00:00:00+01:00</published><updated>2019-10-15T00:00:00+01:00</updated><id>http://localhost:4000/hiking/travel/2019/10/15/snowdonia</id><content type="html" xml:base="http://localhost:4000/hiking/travel/2019/10/15/snowdonia.html"><![CDATA[<p>Few Snaps
<img src="http://localhost:4000/assets/images/snowden/IMG_20191130_102135.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/snowden/IMG_20191130_103008.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/snowden/IMG_20191130_103016.jpg" alt="image" />
<img src="http://localhost:4000/assets/images/snowden/P_20191130_095925.jpg" alt="image" /></p>]]></content><author><name>Anil Kunchala</name></author><category term="[&quot;Travel&quot;]" /><summary type="html"><![CDATA[Another Trip to Snowdonia National Park, Wales, United Kingdom]]></summary></entry></feed>